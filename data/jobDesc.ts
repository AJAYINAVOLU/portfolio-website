export const internshipWorkAtBI = `
## Overview

---

#### 1: Optimizing Data Retrieval with GraphQL
    
At One Community Global, I played a key role in optimizing data retrieval by implementing a GraphQL API layer over existing Spring Boot microservices. The goal was to streamline data fetching and minimize network overhead. By consolidating multiple client requests into a single GraphQL query, I significantly reduced network traffic by 30%. This enhancement not only improved system throughput but also provided a more efficient way for frontend applications to access data while minimizing redundant API calls.

---

#### 2: Transforming a Monolithic System into Scalable Microservices
    
To enhance scalability and maintainability, I led the migration of an employee management system from a monolithic architecture to a containerized microservices-based structure using Spring Boot and OAuth 2.0. This transformation ensured secure authorization and seamless access control while improving system modularity. Additionally, I implemented comprehensive unit tests using JUnit, PowerMock, and Mockito, achieving 95% test coverage and reinforcing system reliability. The migration resulted in a 30% increase in throughput, enabling more efficient and scalable operations.

--- 

#### 3: Enhancing Real-Time Processing with Low-Latency Concurrency

In the Employee Management App, I introduced a high-performance concurrency mechanism for handling real-time task updates. By leveraging Java 17 multithreading and Kafka-based asynchronous processing, I optimized the system to support 200+ concurrent tasks with sub-second performance. This enhancement ensured that critical operations, such as employee task tracking and event-based updates, could be processed with minimal delays, improving both responsiveness and system reliability.

---

#### 4: Automating Financial Workflows with Python

To improve financial management efficiency, I developed Python automation scripts for generating employee timesheets and payroll statements. By implementing batch job processing, I reduced processing time by 15%, eliminating manual errors and streamlining workflow execution. These automation scripts seamlessly integrated with the existing backend, ensuring accurate and timely payroll processing. This automation significantly enhanced financial data accuracy while optimizing resource utilization.

#### Milestones:
  - Reduced network traffic by 30% by implementing a GraphQL API layer for efficient data retrieval.
  - Increased system throughput by 30% by migrating monolithic services to containerized Spring Boot microservices with OAuth 2.0.
  - Achieved sub-second latency for real-time concurrent task processing using Java 17 multithreading and Kafka.
  - Reduced payroll processing time by 15% by developing Python-based automation for financial workflows.
`

export const workExpAtNEU = `
## Overview

---

I was a Teaching Assistant for the Database course under Prof. John Rachlin. During this role, I held office hours to assist students 
with their homework, projects, and conceptual understanding of the material. Since the course was taught using Python, I also provided 
support with Python programming for some students who needed it. My responsibilities included grading assignments and offering 
constructive feedback to help students deepen their understanding of the subject and improve their programming skills.

---
### Key Takeaways
- Developed strong communication skills by explaining complex concepts in simple terms to students with varying levels of understanding.
- Helping students improve their skills and overcome learning challenges taught me the value of patience and personalized mentorship.
- Balancing responsibilities like conducting office hours, grading, and supporting students required effective time and task management.
- Acting as a guide and mentor for students fostered my leadership abilities, preparing me to take on more collaborative and managerial roles in the future.
`

export const workAtJocata = `
## Overview

--- 
### 1: Developing Scalable Microservices for a Credit Scoring Platform
At OMDENA, I engineered containerized RESTful microservices using Spring Boot to power a credit scoring platform hosted on AWS Elastic Kubernetes Service (EKS). The goal was to improve user interactions with credit score attributes while maintaining high availability and scalability. By optimizing API request handling and caching strategies, I boosted API response times by 20% and expanded the user base by 10%, ensuring seamless access to financial data across distributed environments.

---

### 2: Implementing Real-Time Log Analytics with ELK & Kafka

To enhance system monitoring and debugging capabilities, I built a real-time log analytics platform using Elasticsearch, Logstash, Kibana (ELK), and Apache Kafka. This system actively aggregated logs across distributed microservices, providing real-time insights into system performance and errors. By implementing automated log indexing and alerting, I reduced L3 issue resolution time by 30%, enabling the team to proactively detect and resolve anomalies before they impacted system functionality.

---
### 3: Optimizing Database Load with Redis Caching & Rate Limiting
To improve database performance and reduce redundant read operations, I integrated Redis caching and an Express Rate Limiter to handle high-frequency database queries efficiently. Additionally, I implemented OAuth token management for secure endpoint access to external applications, enhancing API security and reducing database read load by 60%. These optimizations significantly improved system scalability while ensuring secure and efficient user authentication.

---
### 4: Automating CI/CD & Reducing Deployment Times
To streamline the software deployment lifecycle, I integrated Jenkins Core Agent into the CI/CD pipeline alongside SonarQube for continuous code quality checks. This enhancement led to a 30% reduction in deployment times, improving code reliability and release efficiency. Additionally, I automated containerized deployments on AWS EC2 instances, ensuring rapid scalability and minimizing downtime during updates.

---
### Milestones
 - Boosted API response times by 20% and expanded the user base by 10% by optimizing microservices for a credit scoring platform.
 - Reduced L3 issue resolution time by 30% by implementing a real-time log analytics system with ELK and Kafka.
 - Reduced database read load by 60% through Redis caching, API rate limiting, and secure OAuth token management.
 - Reduced deployment times by 30% by integrating Jenkins Core Agent with SonarQube, improving CI/CD efficiency.

`

export const workAtIIT = `
## Overview

--- 
### 1: Developing a High-Accuracy Deep Learning Model for Human Activity Recognition
At IIT Indore, I focused on Human Activity Recognition (HAR) using mobile sensor data, where I developed a deep learning model using TensorFlow and Keras. By implementing Convolutional Neural Networks (CNNs) and applying iterative threshold optimization, I significantly improved model precision by 15%, achieving an overall accuracy of 96%. This model effectively classified human activities based on motion sensor data, enhancing applications in healthcare, fitness tracking, and smart environments.

---

### 2: Optimizing Data Pipelines for Efficient Training & Inference
To streamline model training and inference, I designed and implemented high-performance data pipelines with TensorFlow. By optimizing data prefetching, augmentation, and parallel processing, I increased training efficiency by 10% and reduced model training time by 20%. These optimizations ensured that large-scale sensor datasets were processed efficiently, improving the responsiveness and scalability of the system.

---
### 3: Automating Model Retraining with Apache Airflow
To ensure the model adapted to new sensor data, I orchestrated an automated model retraining workflow using Apache Airflow. This system handled data ingestion, preprocessing, model training, and validation in a fully automated manner. As a result, deployment time was reduced by 5%, and 90% of the retraining process was automated, enhancing the systemâ€™s ability to continuously learn and improve without manual intervention.

---
### 4: Scaling Big Data Processing with PySpark & ELK Monitoring
Given the large volume of sensor data (~100GB), I utilized PySpark and Spark/Scala for distributed data processing, improving data ingestion speed by 25%. Additionally, I integrated Elasticsearch, Logstash, and Kibana (ELK Stack) for real-time monitoring, which helped cut error detection time by 30%. These optimizations enabled the system to process and analyze large datasets efficiently, ensuring scalability for real-world applications.

---
### Milestones
 - Achieved 96% accuracy in Human Activity Recognition (HAR) by developing a CNN-based deep learning model in TensorFlow/Keras.
 - Increased training efficiency by 10% and reduced training time by 20% by optimizing TensorFlow data pipelines.
 - Automated 90% of the model retraining process with Apache Airflow, reducing deployment time by 5%.
 - Improved data ingestion speed by 25% and cut error detection time by 30% using PySpark & ELK Stack for real-time monitoring.

`